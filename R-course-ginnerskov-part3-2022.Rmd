---
title: "Text analysis III: Building a tidytext toolbox"
subtitle: "Introduction to R for Social Sciences"
author: 
- "Josef Ginnerskov"
institute: "Department of Sociology"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    self_contained: true # This allows to use the html file without copying all files.
    lib_dir: libs
    css: ["./resources/css/uppsala.css", "./resources/css/uppsala-fonts.css"]
    includes:
      in_header: "./libs/partials/header.html"
#      after_body: "./resources/html/insert-logo.html"
    nature:
      beforeInit: ["./resources/js/macro.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: true
      ratio: "16:9" # If you change it to 4:3, open insert-logo.html to fix the watermark position.
editor_options: 
  chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      message = F,
                      warning = F,
                      dpi = 600,
                      fig.asp = 0.5, # You may change these parameters for each chunk as needed.
                      fig.align="center",
                      out.width = "80%")

library(tidyverse)

#For exporting as pdf, run 'pagedown::chrome_print('R-course-ginnerskov-part3-2022.html')' in the console, where 'html' is the name of html file to be exported as pdf (in this case, 'template.html').

```

# Today's outline

1. A conceptual background to (computational) text analysis in R

2. Basic text analysis tasks performed on vector strings following the logic of the tm package 

3. __More advance text analysis tasks conducted on digitized books based on the tidytext package__

4. Individually solving the problem set by building your own Gutenberg corpus
---

# Loading books from Gutenberg (1/4)
GutenbergR is a package that lets us harvest the many fruits of the digital archive www.gutenberg.org as full texts with metadata. 
```{r, echo = T, eval = F}
library(tidyverse) # load the familiar tidyverse package
library(gutenbergr) # loading the Gutenberg R package

gutenberg_metadata # a tibble of the 51 k works with 8 metadata
gutenberg_authors # a tibble of the 16 k authors with 7 metadata 
gutenberg_works() # a tibble of the 40 k English works with 8 metadata

gutenberg_works(author == "Dostoyevsky, Fyodor") # filter for only English translations of Dostoyevsky

```

--

```{r}
library(tidyverse) # load the familiar tidyverse package
library(gutenbergr) # loading the Gutenberg R package

gutenberg_works(author == "Dostoyevsky, Fyodor") # filter for only works by Dostoyevsky
```

---

# Loading books from Gutenberg (2/4)
To engage in text analysis, we need to build ourselves a test corpus. The code below is set to download six sociology books from www.gutenberg.org.
```{r, echo = T, eval = F}
Socbooks.raw <- gutenberg_download(c(41360, 46423, 13205, 30610, 6568, 21609), # downloading books by id (Durkheim, Marx, Geddes, Blackmar, Ellwood and Rowe)
                                   mirror = "http://mirrors.xmission.com/gutenberg/",# when loading several books at once you might need to state a proper mirror...
                                   meta_fields = c("author", "title"), strip = TRUE) # include author and title metadata; "strip" entails keeping only the main book text

head(Socbooks.raw) # let's see what we got
```

--

```{r}
Socbooks.raw <- gutenberg_download(c(41360, 46423, 13205, 30610, 6568, 21609), # downloading books by id (Durkheim, Marx, Geddes, Blackmar, Ellwood and Rowe)
                                   mirror = "http://mirrors.xmission.com/gutenberg/", # for many books you might need to state a proper mirror...
                                    meta_fields = c("author", "title"), strip = TRUE) # include author and title metadata; "strip" entails keeping only the main book texthead(Socbooks.raw)
head(Socbooks.raw) # let's see what we got
```

---

# Loading books from Gutenberg (3/4)
A first feeling for the volume of the books before proceeding with the tidytext package.
```{r, echo = T, eval = F}
Socbooks.raw %>% # yay, we can pipe it
  count(title) # counting how many rows that are in each book
```

--

```{r}
Socbooks.raw %>%
  count(title)
```

---
# Loading books from Gutenberg (4/4)
Unfortunately, there are often a lot of unwanted signs in old books that will mess up the text analysis and thus need to be removed pronto.
```{r, echo = T, eval = F}

Socbooks.sub <- Socbooks.raw # to keep things in order we create an object for substituting odd signs

Socbooks.sub$text <- gsub("_","",as.character(Socbooks.sub$text)) # substitute _ for nothing in the text column
Socbooks.sub$text <- gsub("--"," ",as.character(Socbooks.sub$text)) # substitute -- for a blank space in the text column
```

--

```{r}
Socbooks.sub <- Socbooks.raw # to keep things in order we create an object for substituting odd signs

Socbooks.sub$text <- gsub("_","",as.character(Socbooks.sub$text)) # substitute _ for nothing in the text column
Socbooks.sub$text <- gsub("--"," ",as.character(Socbooks.sub$text)) # substitute -- for a blank space in the text column
```
---

# Turning books into a tidy tibble
In this analysis, we are goind to work with tidytext, which is a package set out to make text analysis friendly for the tidyverse universe. This is great since we can then add on other tidyverse packages like ggplot2 (you can later return to Pablo Lillo Cea's lecture on visualization to refresh your ggplot skills).
```{r, echo = T, eval = F}
library(tidytext) # load tidytext
Socbooks.tidy <- Socbooks.sub %>% # we work with the substituted corpus
    mutate(line = row_number()) %>% # by mutating, we can keep information on what line a word resides
    unnest_tokens(word, text) # turning the book into a tall tibble where each word or token makes up a row (also preprocesses)
                              # can be modified to split by n-grams, sentences, lines, paragraphs, tweets...
head(Socbooks.tidy) # so, what happend?
```

--

```{r}
library(tidytext)
Socbooks.tidy <- Socbooks.sub %>%
    mutate(line = row_number()) %>%
    unnest_tokens(word, text)

head(Socbooks.tidy)
```
---
# Removing stop words
For most text analysis tasks, words that are too common do not only hold no meaning but are disruptive and cause an obstacle for performing the methods. 
```{r, echo = T, eval = F}
Socbooks.tidy.stop <- Socbooks.tidy %>% # tidytext comes with its own list "stop_words" that in most cases works great
  anti_join(stop_words) # anti_join from dplyr can be used to remove these words

Socbooks.stop.words <- tibble(word = # as expected, some academic slang ought to be removed as well
                                c("p", "cit", "tr", "pp", "ff", "nat", "ibid", "geddes", "prof", "per"))

Socbooks.tidy.stop <- Socbooks.tidy.stop %>% # remove your own set of stop words from the filtered tibble
  anti_join(Socbooks.stop.words)
```

--

```{r}
Socbooks.tidy.stop <- Socbooks.tidy %>%
  anti_join(stop_words)

Socbooks.stop.words <- tibble(word = c("p", "cit", "tr", "pp", "ff", "nat", "ibid", "geddes", "prof", "per"))

Socbooks.tidy.stop <- Socbooks.tidy.stop %>%
  anti_join(Socbooks.stop.words)
```

---
# Basic global/corpus count
With our tidy corpus at hand we can begin the analysis. Let's look at the top words for the corpus.
```{r, echo = T, eval = F}
Socbooks.tidy.stop %>%
    count(word, sort = TRUE) %>% # we count the words and sort the counts from high to low
    top_n(10) # since this will be overwhelming we only look at the top 10
```

--

```{r}
Socbooks.tidy.stop %>%
    count(word, sort = TRUE) %>%
    top_n(10)
```

---
# Generating a word cloud (1/2)
In the previous lecture, we saw that word clouds can bring some visual insights into the data, so why don't we generate a word cloud for the whole corpus?
```{r, echo = T, eval = F}
library(wordcloud) # there is a specific package for word clouds - load it!

Socbooks.tidy.stop %>% # piping the clean tibble
  count(word) %>% # count all words
  with(wordcloud(word, n, # adding on the base R command "with"
                 max.words = 100, # limit the cloud to 100 words
                 colors = brewer.pal(8, "Dark2"))) # add our favorite color palette
```
---
# Generating a word cloud (2/2)
```{r}
library(wordcloud)

Socbooks.tidy.stop %>%
  count(word) %>%
  with(wordcloud(word, n, 
                 max.words = 100, 
                 colors = brewer.pal(8, "Dark2")))
```

---
# Basic local/document count
Often you would quickly want to move from the global corpus level to the local document level. We can do the same count as previously but filter for a specific book or author and so on.
```{r, echo = T, eval = F}
Socbooks.tidy.stop %>%
    filter(author == "Marx, Karl") %>% # filter by author, specifically Marx
    count(word, sort = TRUE) %>% # again, count words and sort from high to low
    top_n(10) # only the top 10
```

--

```{r}
Socbooks.tidy.stop %>%
    filter(author == "Marx, Karl") %>%
    count(word, sort = TRUE) %>%
    top_n(10)
```

---
# Comparing word occurences per book
The next natural step is to compare the books. With the code below we can check out the most frequently used words book by book. 
```{r, echo = T, eval = F}
Socbooks.words <- Socbooks.tidy.stop %>% # we will need to store a separate object with the counts
  count(author, word, sort = TRUE) # including all words and the number of times each author have used them

Socbooks.word.count <- Socbooks.words %>% # also a separate object with the total words for each work
  group_by(author) %>% # first group by author
  summarize(total = sum(n)) # second summarize the total word could

Socbooks.word.count <- left_join(Socbooks.word.count, Socbooks.words) # finally join the words by author with the total words of each author
```

--

```{r}
Socbooks.words <- Socbooks.tidy.stop %>%
  count(author, word, sort = TRUE)

Socbooks.word.count <- Socbooks.words %>% 
  group_by(author) %>% 
  summarize(total = sum(n))

Socbooks.word.count <- left_join(Socbooks.word.count, Socbooks.words)

Socbooks.word.count
```
---
# Viz word occurences per book (1/2)
Let us visualize the scores with the help of the ggplot2 package.
```{r, echo = T, eval = F}
Socbooks.word.count %>%
   filter (n >= 333) %>% # filter so that only the 333 most used words are shown
   mutate(word = reorder(word, n)) %>% # reorder by word after the highest count "n"
   ggplot(aes(x = word, y = n, fill = author)) + # plot with the word in x, count in y and fill by author
   geom_col() + # bar chart where the heights of the bars represent the data values
   coord_flip() # since y is conditional on x, we can flip them for the sake of interpretation
```
---
# Viz word occurences per book (2/2)

```{r}
Socbooks.word.count %>% filter (n >= 333) %>% 
   mutate(word = reorder(word, n)) %>% 
   ggplot(aes(x = word, y = n, fill = author)) + 
   geom_col() + 
   coord_flip()
```
---
# Comparing books via tf-idf
To get a better idea of which words are the most distinct for each book in relation to the overall corpus, term frequency-inverse document frequency always comes in handy.
```{r, echo = T, eval = F}
Socbooks.tfidf <- Socbooks.tidy.stop %>% # new object storing the tf-idf statistics
    count(author, word, sort = TRUE) %>% # count words by author and sort the results
    bind_tf_idf(word, author, n) %>% # this tidytext function runs the tf-idf by word, author and counts (n)
    arrange(-tf_idf) %>% # orders the rows of by the values of the tf-idf score
    group_by(author) %>% # group the results by author
    top_n(15) %>% # keep only the top 15 words for each document
    ungroup() # ungroup the author grouping
```

```{r}
Socbooks.tfidf <- Socbooks.tidy.stop %>%
    count(author, word, sort = TRUE) %>%
    bind_tf_idf(word, author, n) %>%
    arrange(-tf_idf) %>%
    group_by(author) %>%
    top_n(15) %>%
    ungroup()
Socbooks.tfidf
```
---
# Viz books via tf-idf (1/2)
To facilitate our ability to interpret the results, it is a good idea to visualize the tf-idf scores.  
```{r, echo = T, eval = F}
Socbooks.tfidf %>%
    mutate(word = reorder_within(word, tf_idf, author)) %>% # mutate and reorder by the words and their tf-idf scores by author
    ggplot(aes(word, tf_idf, fill = author)) + # the words as x and tf-idf socre as y, fill by author
    geom_col(alpha = 0.8, show.legend = FALSE) + # a bar chart; let's set the opacity (alpha) to 80% and drop the legend since we do not need it
    facet_wrap(~ author, scales = "free", ncol = 3) + # to fit the plot within the screen space, we wrap by author in three free scaled columns
    scale_x_reordered() + # we order the x axis along the facets defined in the facet_wrap
    coord_flip() + # again, flip x and y for the sake of interpretation
    theme(strip.text=element_text(size=11)) # add a nicer sized font
```
---
# Viz books via tf-idf (2/2)

```{r}
Socbooks.tfidf %>%
    mutate(word = reorder_within(word, tf_idf, author)) %>%
    ggplot(aes(word, tf_idf, fill = author)) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ author, scales = "free", ncol = 3) +
    scale_x_reordered() +
    coord_flip() +
    theme(strip.text=element_text(size=11))
```

---
# Calculating word correlations
To get some insights into how the most distinct words in the corpus are interrelated, we can calculate word correlations. Correlations were covered earlier in the course by Anna Soloveva.
```{r, echo = T, eval = F}
library(widyr) # load package for computing correlations, co-occurrences etc on tidy data
Socbooks.word.corr <- Socbooks.word.count %>% # reuse the object for author word counts
  group_by(word) %>% # group by the variable "word"
  filter(n >= 250) %>% # only keep words occurring at least 250 times
  pairwise_cor(word, author, method = "pearson", sort = TRUE, upper = FALSE) # calculating the Pearson correlation coefficient; sort; avoid duplicates 
```

--

```{r}
library(widyr) 
Socbooks.word.corr <- Socbooks.word.count %>% 
  group_by(word) %>%
  filter(n >= 250) %>%
  pairwise_cor(word, author, sort = TRUE, upper = FALSE,  method = "pearson")

Socbooks.word.corr
```
---

# Generate a word correlation graph (1/3)
Now let us try to visualize the results with a word graph. The first step is to turn our word correlations into edges.
```{r, echo = T, eval = F}
library(igraph) # load package for constructing graphs
set.seed(1234) # pick a seed number to get the same results for randomization
Socbooks.word.corr.graph <- Socbooks.word.corr %>%
  filter(correlation > .3) %>% # filter to avoid overflowing the graph with insignificant words
  graph_from_data_frame() # turning our word correlations into a graph
```

--

```{r}
library(igraph)
set.seed(1234)
Socbooks.word.corr.graph <- Socbooks.word.corr %>%
  filter(correlation > .3) %>%
  graph_from_data_frame()
Socbooks.word.corr.graph
```

---
# Generate a word correlation graph (2/3)
The second step is to create a nice visualization based on the word correlation graph that is easy to interpret.
```{r, echo = T, eval = F}
library(ggraph) # an extension of ggplot2 aimed at supporting relational data structures
Socbooks.word.corr.graph %>% 
  ggraph() + # function for generating a graph; you can try several different layouts with argument layout = "fr" [alt. "kk" or "drl"...)
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "orange") + # the edges/links are taken from the correlation variable; choose a cool color
  geom_node_point(size = 0.5*igraph::degree(Socbooks.word.corr.graph), colour = "lightblue") + # size the nodes/points based on half the number of its adjacent edges; color it
  geom_node_text(aes(label = name), repel = TRUE) + # display the word associated with each node/point; repel to enable interpretation
  theme_void() # add a haunting theme to let the graph elevate
```

---
# Generate a word correlation graph (3/3)
```{r}
library(ggraph)
Socbooks.word.corr.graph %>% 
  ggraph() +
  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = "orange") +
  geom_node_point(size = 0.5*igraph::degree(Socbooks.word.corr.graph), colour = "lightblue") +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()
```

---
# Preparing for topic modeling
You might also be interested in modeling the most prevalent topics that run through your corpus, which can be made with topic modeling. One way to define topic modeling is to say that it is able to discover abstract topics within a corpus with the help of a probabilistic unspervised machine learning model. Step one for topic modeling is to create a document-term matrix (dtm).
```{r, echo = T, eval = F}
Socbooks.dtm <- Socbooks.word.count %>% # we reuse the word counts in a dtm
  cast_dtm(author, word, n) # tidytext provides several fine cast functions to enable fast conversions
```

--

```{r}
Socbooks.dtm <- Socbooks.word.count %>%
  cast_dtm(author, word, n)

Socbooks.dtm
```
---
# Run a topic model
Now we are ready for topic modeling. This time we will go for the Latent Dirichlet Allocation (LDA), a classic and still most popular topic model. 
```{r, echo = T, eval = F}
library(topicmodels) # a package for LDA and correlated topic models (CTM) by its creators
Socbooks.lda <- LDA(Socbooks.dtm, # function for generating a LDA
                    k = 4, # in topic modeling you have to select the number of topics (k); this can be tricky but for now let's go for 4
                    control = list(seed = 1234)) # again, to get the same results for randomization
```

--

```{r}
library(topicmodels)
Socbooks.lda <- LDA(Socbooks.dtm, k = 4, control = list(seed = 1234))
Socbooks.lda
```
---
# Inspecting the beta
In a topic model, the beta tells us how probable it for words to show up in a topic.
```{r, echo = T, eval = F}
Socbooks.lda.beta <- tidy(Socbooks.lda, matrix = "beta") # the tidy function can easily generate a beta matrix
```

--

```{r}
Socbooks.lda.beta <- tidy(Socbooks.lda, matrix = "beta")
Socbooks.lda.beta
```
---
# Top terms in topic model beta
To interpret the topics, we are interested in the words with the highest probability for each topic.
```{r, echo = T, eval = F}
Socbooks.lda.beta.top10 <- Socbooks.lda.beta %>%
  group_by(topic) %>% # we group by the 4 topics
  slice_max(beta, n = 10) %>% # for each we keep the 10 words with the largest beta value
  ungroup() %>% # ungroup the grouping
  arrange(topic, -beta) # arrange the scores by topic and beta
```

--

```{r}
Socbooks.lda.beta.top10 <- Socbooks.lda.beta %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
Socbooks.lda.beta.top10
```
---
# Viz topic model beta (1/2)
Let us visualize the top words, topic by topic.
```{r, echo = T, eval = F}
library(ggplot2)

Socbooks.lda.beta.top10 %>%
  mutate(term = reorder_within(term, beta, topic)) %>% # order terms after their beta values in each topic
  ggplot(aes(beta, term, fill = factor(topic))) + # beta values as x, terms as y and fill by the topics as factors
  geom_col(show.legend = FALSE) + # bar chart without legend
  facet_wrap(~ topic, scales = "free") + # free scale wrap after topic
  scale_y_reordered() # order after terms
```
---
# Viz topic model beta (2/2)

```{r}
library(ggplot2)

Socbooks.lda.beta.top10 %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

---
# Inspecting the gamma
In a topic model, the gamma tells us how probable it for topics to show up in a document.
```{r, echo = T, eval = F}
Socbooks.lda.gamma <- tidy(Socbooks.lda, matrix = "gamma") # the tidy function can easily generate a gamma matrix
```

--

```{r}
Socbooks.lda.gamma <- tidy(Socbooks.lda, matrix = "gamma")
Socbooks.lda.gamma
```
---
# Viz topic model gamma (1/2)
Let us visualize how the topics and the books relate to one another.
```{r, echo = T, eval = F}
Socbooks.lda.gamma %>%
  mutate(document = reorder(document, gamma * topic)) %>% # order the books after their gamma values vis-a-vis each topic
  ggplot(aes(factor(topic), gamma)) + # factor topics
  geom_boxplot() + # choose box plot for easy comparison
  facet_wrap(~ document)
```
---
# Viz topic model gamma (2/2)

```{r}
Socbooks.lda.gamma %>%
  mutate(document = reorder(document, gamma * topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ document)
```
---
# Adding sentiments to your analysis
Lastly, you might be interested in a more supervised approach, such as investigating the emotional architecture of your documents. This leads us to sentiment analysis. 
```{r, echo = T, eval = F}
bing <- get_sentiments("bing") # tidytext provides you with sentiment lexicons like the famous Bing

Socbooks.senti.count <- Socbooks.tidy.stop %>% # go back to our tidy corpus
  inner_join(bing) %>% # connect it with the word lexicon and its positive/negative sentiment system
  count(word, sentiment, sort = TRUE) %>% # count words by their sentiment and sort
```

--

```{r}
bing <- get_sentiments("bing")

Socbooks.senti.count <- Socbooks.tidy.stop %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)
Socbooks.senti.count
```
---
# Viz top sentiments (1/2)
We can now generate a visualization to get a feeling for the overall distribution of sentimental words over the corpus.
```{r, echo = T, eval = F}
Socbooks.senti.count %>%
  filter(n > 100) %>% # keep only the most prevalent words
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) + # plot the words and color them by sentiment
  geom_col() + # go for a bar chart
  coord_flip() # flip the axes
```
---
# Viz top sentiments (2/2)
```{r}
Socbooks.senti.count %>%
  filter(n > 100) %>% # keep only the most prevalent words
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) + # plot the words and color them by sentiment
  geom_col() + # go for a bar chart
  coord_flip() # flip the axes
```
---
# The sentiment narrative unfolds (1/3)
As often is the case, we might not only want to compare how emotional each document is but also how it develops over the pages. This is possible.
```{r, echo = T, eval = F}
Socbooks.senti.comp <- Socbooks.tidy.stop %>%
  inner_join(bing) %>% # connect the tidy corpus with the sentiment lexicon
  count(author, index = line %/% 100, sentiment) %>% # count sentiment by author and create an index based on the book lines
  spread(sentiment, n, fill = 0) %>% # increasing the number of columns and decreasing the number of rows
  mutate(sentiment = positive - negative) # add a new sentiment variable
```

--

```{r}
Socbooks.senti.comp <- Socbooks.tidy.stop %>%
  inner_join(bing) %>%
  count(author, index = line %/% 100, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
Socbooks.senti.comp
```

---
# The sentiment narrative unfolds (2/3)
So, let us visualize the comparison for all books.
```{r, echo = T, eval = F}
Socbooks.senti.comp %>%
  ggplot(aes(index, sentiment, fill = author)) + # we use our index as x and sentiment as y and color by author
  geom_bar(stat = "identity", show.legend = FALSE) + # bar chart with an identified y value ("identity") without the legend
  facet_wrap(~author, ncol = 2, scales = "free_x") # we wrap by author in two free scaled columns
```
---
# The sentiment narrative unfolds (3/3)

```{r}
Socbooks.senti.comp %>%
  ggplot(aes(index, sentiment, fill = author)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~author, ncol = 2, scales = "free_x")
 
```


---

# Thank you for your time!

## Do not hesitate to contact me

|                                                                                              |                          |
|:---------------------------------------------------------------------------------------------|:-------------------------|
| <a href="mailto:josef.ginnerskov@soc.uu.se">.UUred[<i class="fa fa-paper-plane fa-fw"></i>] |josef.ginnerskov@soc.uu.se |  
| <a href="http://twitter.com/doeparen">.UUred[<i class="fa fa-twitter fa-fw"></i>]         |@doeparen              |
| <a href="http://github.com/doeparen">.UUred[<i class="fa fa-gitlab fa-fw"></i>]           |@doeparen              |